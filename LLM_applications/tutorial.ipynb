{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11024f8",
   "metadata": {},
   "source": [
    "# Step1. Generate the examples from LLMs.\n",
    "\n",
    "The interpretation model aims to explain how LLMs generate tokens. To understand the behavior of each LLM, we first need to collect the corpus reflecting how the model interprets each question, and then train transcoders to extract the corresponding interpretive structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04342a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': 'Mary is baking a cake . The recipe wants N_00 cups of flour . She already put in N_01 cups . How many cups does she need to add ?', 'Equation': 'N_00 - N_01', 'Answer': 6.0, 'Numbers': '8.0 2.0'}\n",
      "1772\n",
      "Loaded pretrained model Qwen/Qwen3-0.6B into HookedTransformer\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "import config_env\n",
    "import os\n",
    "        \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from model_load import load_model, model_name_func\n",
    "\n",
    "    \n",
    "name = 'Qwen3-0.6B'\n",
    "dataset_name = 'maw'\n",
    "model_name =  model_name_func(name)# 'meta-llama/Llama-3.2-1B'# \"Qwen/Qwen2.5-1.5B\"\n",
    "dataset = load_dataset(\"mwpt5/MAWPS\")\n",
    "test = dataset[\"train\"]\n",
    "print(dataset['train'][0])\n",
    "print(len(test))\n",
    "model = load_model(name)\n",
    "print(model.cfg.n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0a8233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1055f4fe12434d5db8577850ba814c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Mary is baking a cake . The recipe wants 8.0 cups of flour . She already put in 2.0 cups . How many cups does she need to add ?\n",
      "Let's think step by step\n",
      "Answer:\n",
      "To find out how many cups Mary needs to add, we start with the total amount of flour required by the recipe, which is 8.0 cups. She has already put in 2.0 cups, so we subtract that from the total. \n",
      "\n",
      "8.0 cups (recipe) - 2.0 cups (already added) = 6.0 cups. \n",
      "\n",
      "Therefore, Mary needs to add 6.0 cups of flour.\n",
      "Answer: 6.0\n",
      "\n",
      "Step-by-Step Explanation:\n",
      "1. The recipe requires 8.0 cups of flour.\n",
      "2. Mary has already added 2.0 cups.\n",
      "3. To find the remaining amount needed, subtract the already added from the required total: 8.0 - 2.0 = 6.0.\n",
      "4. The answer is 6.0 cups.\n",
      "Answer: 6.0\n",
      "The answer is 6.0 cups.\n",
      "Answer: 6.0\n",
      "The answer is  6.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from generate_supposed_answers import generate_MAWPS\n",
    "text = []\n",
    "for idx,data in enumerate(test):\n",
    "    question, prompt, ans, max_token = generate_MAWPS(data)\n",
    "    generated_text = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=max_token,  \n",
    "        temperature=0,     \n",
    "    )\n",
    "    print(generated_text, ans)\n",
    "    text.append({'question': question, 'gold_ans': ans, 'ans':generated_text})\n",
    "    print(idx)\n",
    "    break\n",
    "# in jupyter note book we only run one example, to run the whole dataset, please refer generate_supposed_answer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af24e2",
   "metadata": {},
   "source": [
    "# Step2. Train transcoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae02a0",
   "metadata": {},
   "source": [
    "### Step2.1 set the backbone model and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a31e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen3-0.6B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1772/1772 [00:00<00:00, 3459.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3239.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import json\n",
    "from model_load import load_model, model_name_func\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.examples = []\n",
    "        self.token_count = 0\n",
    "        for text in tqdm(texts, total=len(texts)):\n",
    "            ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            self.examples.append(torch.tensor(ids).long())\n",
    "            self.token_count += len(ids)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.examples:\n",
    "            yield {\"tokens\": item} \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "device = 'cuda:0'\n",
    "acts_func = 'relu'\n",
    "train_dataset_name = 'maw'\n",
    "from_pretrained = False\n",
    "name = 'Qwen3-0.6B'\n",
    "model_name = model_name_func(name)\n",
    "model = load_model(name, device=device)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "save_json = f'./data_{train_dataset_name}/{name}_answer.json'\n",
    "with open(save_json,'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "training_corpus = []\n",
    "for data in test_data:\n",
    "    training_corpus.append(data['ans'])\n",
    "valid_corpus = training_corpus[:10]\n",
    "\n",
    "\n",
    "train_dataset = MyIterableDataset(training_corpus, tokenizer)\n",
    "valid_dataset = MyIterableDataset(valid_corpus, tokenizer)\n",
    "\n",
    "transcoder_save_path = f'/egr/research-dselab/shared/transcoder_model/{acts_func}_{name}_{train_dataset_name}'\n",
    "activate_cache_dir = '/egr/research-dselab/shared/daixinna/huggingface_path/activations_cache'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224dd6f",
   "metadata": {},
   "source": [
    "### Step2.2 set the parameters for transcoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e26691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('../')\n",
    "from circuit_tracer.configs import Configs\n",
    "from circuit_tracer.transcoder.activation_functions import JumpReLU, Relu, TopK\n",
    "from circuit_tracer.transcoder.single_layer_transcoder import SingleLayerTranscoder\n",
    "from circuit_tracer.transcoder.activations_store import ActivationsStore\n",
    "from circuit_tracer.transcoder_training import train_sae_on_language_model\n",
    "\n",
    "\n",
    "train_dataset_name = 'maw'\n",
    "l1_co = 0.00005\n",
    "max_length = 200\n",
    "total_training_tokens = 5_000_000\n",
    "\n",
    "n_layer = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "\n",
    "acts_func = 'relu'\n",
    "transcoder_config = {}\n",
    "transcoder_config['model_name'] = model_name\n",
    "transcoder_config['train_dataset_name'] = train_dataset_name\n",
    "transcoder_config['dataset_path'] = None\n",
    "transcoder_config['pattern_name'] = 'maw'\n",
    "transcoder_config['method'] = 'real'\n",
    "transcoder_config['is_tokenized'] = True\n",
    "transcoder_config['max_length'] = max_length\n",
    "transcoder_config['total_train_num'] = total_training_tokens\n",
    "transcoder_config['acts_func'] = acts_func\n",
    "\n",
    "# turn epoch if the training set is small while MSE still high\n",
    "transcoder_config['epoch'] = 1\n",
    "transcoder_config['batch_size'] = 1024\n",
    "transcoder_config['train_batch_size'] = 1024\n",
    "transcoder_config['resample_batches'] = 1024\n",
    "# turn this parameters when out of memory (very efficient)\n",
    "transcoder_config['n_batches_in_buffer'] = 32\n",
    "transcoder_config['store_batch_size'] = 16\n",
    "# turn this parameter to balance the MSE and L1 loss\n",
    "transcoder_config['l1_coefficient'] = l1_co\n",
    "\n",
    "# decompose the features to sparse space (can turn the parameters)\n",
    "transcoder_config['d_transcoder'] = d_model * 2\n",
    "\n",
    "# large dead feature window is efficient when large vocab size\n",
    "transcoder_config['dead_feature_window'] = d_model\n",
    "transcoder_config['dead_feature_threshold'] = 1e-8\n",
    "# do not be too small. Better increasing training num instead of large lr\n",
    "transcoder_config['lr'] = 1e-3\n",
    "\n",
    "base_model_configs = {}\n",
    "base_model_configs['d_model'] = d_model\n",
    "base_model_configs['max_length'] = max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f48b9",
   "metadata": {},
   "source": [
    "### Step2.3 Train Transcoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2954e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/egr/research-dselab/daixinna/graphghost/LLM_applications/../circuit_tracer/transcoder/activations_store.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4882| MSE Loss 0.001 | L1 0.000: : 5000192it [01:24, 58987.09it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /egr/research-dselab/shared/transcoder_model/relu_Qwen3-0.6B_maw/0.pt\n",
      "Model saved to /egr/research-dselab/shared/transcoder_model/relu_Qwen3-0.6B_maw/0.pt\n",
      "Target Layer: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for target_layer in range(0, n_layer):\n",
    "    configs = Configs.init_setup(target_layer, transcoder_config, base_model_configs, transcoder_save_path, device, activate_cache_dir)\n",
    "    model = model.to(configs.act_store_device)\n",
    "    configs.tokenizer_name = None\n",
    "    \n",
    "    activation_stores = ActivationsStore(configs, model, dataset=train_dataset, tokenizer=tokenizer)\n",
    "\n",
    "    if acts_func == 'relu':\n",
    "        transcoder = SingleLayerTranscoder(configs, Relu())# JumpReLU(0.0, 0.1))\n",
    "\n",
    "    record_scores = train_sae_on_language_model(configs, model, transcoder, activation_stores, use_eval=False)\n",
    "    print(f\"Target Layer: {target_layer}\")\n",
    "    break\n",
    "# Note, in jupyter file we only run 1 layer. For the whole training please use train_transcoder.py file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5ce26",
   "metadata": {},
   "source": [
    "# Step3. collect the attribution graphs from each samples\n",
    "\n",
    "(This step will cost a lot of computation resource due to the reasoning chain could be very long)\n",
    "\n",
    "Basic idea:\n",
    "\n",
    "We know that an LLM ultimately produces a final answer, while the intermediate tokens form the thinking chain. Therefore, we trace backward from the final token to examine the thinking chain and identify which tokens meaningfully contributed to the final prediction.\n",
    "\n",
    "The generated tokens always depend on the given question. Thus, the backward tracing process stops once the contributing tokens originate from the input question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6802d6",
   "metadata": {},
   "source": [
    "### Step3.1 setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0804c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen3-0.6B into HookedTransformer\n",
      "Loaded pretrained model Qwen/Qwen3-0.6B into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReplacementModel(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (q_norm): RMSNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (k_norm): RMSNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): ReplacementMLP(\n",
       "        (old_mlp): GatedMLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_pre_linear): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_in): HookPoint()\n",
       "        (hook_out): HookPoint(\n",
       "          (hook_out_grad): HookPoint()\n",
       "        )\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): ReplacementUnembed(\n",
       "    (old_unembed): Unembed()\n",
       "    (hook_pre): HookPoint()\n",
       "    (hook_post): HookPoint()\n",
       "  )\n",
       "  (transcoders): ModuleList(\n",
       "    (0-27): 28 x SingleLayerTranscoder(\n",
       "      (activation_function): Relu()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config_env\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from model_load import load_model, model_name_func\n",
    "import networkx as nx\n",
    "sys.path.append('../')\n",
    "from circuit_tracer.replacement_model import ReplacementModel\n",
    "from circuit_tracer.configs import Configs\n",
    "from circuit_tracer.graph import Graph, prune_graph\n",
    "from circuit_tracer import attribute\n",
    "from circuit_tracer.utils.create_graph_files import create_nodes, create_used_nodes_and_edges\n",
    "\n",
    "# controlling the density of attribution graph\n",
    "edge_ratio = 0.5\n",
    "node_ratio = 0.4\n",
    "\n",
    "name = 'Qwen3-0.6B'\n",
    "cache_root = \"/egr/research-dselab/daixinna/shared/huggingface_path\"\n",
    "# use this to setup multi gpu usasge\n",
    "cuda_range = [1]\n",
    "cuda_list = [f'cuda:{i}' for i in cuda_range]\n",
    "\n",
    "model_name = model_name_func(name)\n",
    "model = load_model(name, device=f'cuda:{cuda_range[0]}')\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "n_layer = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "split_layer = int(n_layer/len(cuda_range))\n",
    "\n",
    "plan = []\n",
    "for s, cuda_id in enumerate(cuda_range):\n",
    "    end = (s + 1)*split_layer\n",
    "    if end>n_layer:\n",
    "        end = n_layer\n",
    "    plan.append((torch.device(f'cuda:{cuda_id}'),range(s*split_layer,end)))\n",
    "\n",
    "device_map = {}\n",
    "for dev, layers in plan:\n",
    "    for L in layers:\n",
    "        device_map[L] = dev\n",
    "\n",
    "acts_func = 'relu'\n",
    "train_dataset_name = 'maw'\n",
    "\n",
    "transcoder_save_path = f'/egr/research-dselab/shared/transcoder_model/{acts_func}_{name}_{train_dataset_name}'\n",
    "transcoder_model_save_path = f'/egr/research-dselab/shared/transcoder_model/{acts_func}_{name}_{train_dataset_name}'\n",
    "\n",
    "transcoder_config_path = os.path.join(transcoder_save_path,'config.json')\n",
    "configs = Configs.init_load(transcoder_config_path)\n",
    "configs.set_device_map(device_map)\n",
    "\n",
    "del model\n",
    "model = ReplacementModel.from_self_pretrained_and_transcoders(cfg=configs,model_name=name, model_path = cache_root, transcoders_path = transcoder_model_save_path)\n",
    "model.forward = model.forward_sharded\n",
    "model.set_device_map(device_map)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6768871",
   "metadata": {},
   "source": [
    "### Step3.2. Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a49bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json = f'./data_{configs.pattern_name}/{name}_answer.json'\n",
    "with open(save_json,'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "\n",
    "match_collect = {}\n",
    "for idx, data in enumerate(test_data):\n",
    "    text = str(data['gold_ans'])\n",
    "    match_collect[idx] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfa9a3",
   "metadata": {},
   "source": [
    "### Step3.3 Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c284882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_node_ids(graph, node_id, given_type):\n",
    "    node_id = node_id.split('_')\n",
    "    if given_type == 'embedding':\n",
    "        vocab_id = int(node_id[1])\n",
    "        pos = int(node_id[2])\n",
    "        id = tokenizer.decode(vocab_id)\n",
    "        layer = 'Emb'\n",
    "    if given_type == \"mlp reconstruction error\":\n",
    "        _, layer, pos = int(node_id[0]), int(node_id[1]), int(node_id[2])\n",
    "        id = 'block_inner'\n",
    "    if given_type == \"logit\":\n",
    "        layer, vocab_id, pos = int(node_id[0]), int(node_id[1]), int(node_id[2])\n",
    "        id = tokenizer.decode(vocab_id)\n",
    "    if given_type == \"cross layer transcoder\":\n",
    "        layer, feat_idx, pos = int(node_id[0]), int(node_id[1]), int(node_id[2])\n",
    "        id = tokenizer.decode(graph.input_tokens[pos])\n",
    "    return id, pos, layer\n",
    "\n",
    "def nodes_at_n_hop(G, source, n, filter = ''):\n",
    "    current_layer = set([source])\n",
    "    all_traversed = set([source])\n",
    "    for _ in range(n):\n",
    "        next_layer = set()\n",
    "        for node in current_layer:\n",
    "            if node not in G:\n",
    "                continue\n",
    "            next_layer.update(G.successors(node))\n",
    "            all_traversed.update(G.successors(node))\n",
    "        current_layer = next_layer\n",
    "    if filter != '':\n",
    "        all_traversed = {node for node in all_traversed if node.split('_')[1] == filter}\n",
    "    return all_traversed\n",
    "\n",
    "def implicit_route(prompt_str, ans_pos):\n",
    "    graph = attribute(\n",
    "            prompt=prompt_str,\n",
    "            model=model,\n",
    "            # input_ids=input_ids,\n",
    "            input_ids=None,\n",
    "            max_n_logits=max_n_logits,\n",
    "            desired_logit_prob=desired_logit_prob,\n",
    "            batch_size=batch_size,\n",
    "            max_feature_nodes=max_feature_nodes,\n",
    "            offload=offload,\n",
    "            verbose=verbose,\n",
    "            print_log=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "    node_threshold = node_ratio\n",
    "    edge_threshold = edge_ratio# 0.99\n",
    "\n",
    "\n",
    "    node_mask, edge_mask, cumulative_scores = (\n",
    "        el.cpu() for el in prune_graph(graph, node_threshold, edge_threshold)\n",
    "    )\n",
    "    scan = graph.scan\n",
    "    nodes = create_nodes(graph, node_mask, tokenizer, cumulative_scores, scan)\n",
    "    used_nodes, used_edges = create_used_nodes_and_edges(graph, nodes, edge_mask)\n",
    "    nodes_dicts = {}\n",
    "    for n in used_nodes:\n",
    "        nodes_dicts[n.node_id] = {}\n",
    "        nodes_dicts[n.node_id]['features'] = n.feature_type\n",
    "        nodes_dicts[n.node_id]['clerp'] = n.clerp\n",
    "    \n",
    "    logit_nodes = []\n",
    "    result_graph = {}\n",
    "    for e in used_edges:\n",
    "        if nodes_dicts[e['source']]['features'] == \"mlp reconstruction error\": continue\n",
    "        if nodes_dicts[e['target']]['features'] == \"mlp reconstruction error\": continue\n",
    "        if nodes_dicts[e['target']]['features'] == \"logit\" and nodes_dicts[e['source']]['features'] == 'embedding': continue\n",
    "        # if nodes_dicts[e['source']]['features'] == \"cross layer transcoder\":\n",
    "        weights = e['weight']\n",
    "        if weights<0. :continue\n",
    "        source_node, source_pos, source_layer = translate_node_ids(graph, e['source'], nodes_dicts[e['source']]['features'])\n",
    "        target_node, target_pos, target_layer = translate_node_ids(graph, e['target'], nodes_dicts[e['target']]['features'])\n",
    "        # print(f\"Edge from {source_node} {e['source']} ({nodes_dicts[e['source']]['features']}, pos: {source_pos}, layer: {source_layer}) to {target_node} {e['target']} ({nodes_dicts[e['target']]['features']}, pos: {target_pos}, layer: {target_layer}) with weight {weights}\")\n",
    "        \n",
    "        start_node = f'{source_node}_{source_layer}_{source_pos}'\n",
    "        end_node = f'{target_node}_{target_layer}_{target_pos}'\n",
    "        if end_node not in result_graph:\n",
    "            result_graph[end_node] = {}\n",
    "        if start_node not in result_graph[end_node]:\n",
    "            result_graph[end_node][start_node] = 0\n",
    "        result_graph[end_node][start_node] += weights\n",
    "        \n",
    "        if nodes_dicts[e['target']]['features'] == 'logit':\n",
    "            logit_nodes.append(end_node)\n",
    "    \n",
    "    result_graph = {outer_k: {inner_k: inner_v \n",
    "            for inner_k, inner_v in outer_v.items() if inner_v > 0} \n",
    "    for outer_k, outer_v in result_graph.items()}\n",
    "    result_graph = {k: v for k, v in result_graph.items() if v}\n",
    "    result_graph_G = nx.DiGraph()\n",
    "    \n",
    "    for key in result_graph.keys():\n",
    "        for inner_key in result_graph[key].keys():\n",
    "            result_graph_G.add_edge(key, inner_key, weight=result_graph[key][inner_key])\n",
    "    \n",
    "    marked_pos = []\n",
    "    repeat_check = set()\n",
    "    for target_node in list(set(logit_nodes)):    \n",
    "        target_layers = nodes_at_n_hop(result_graph_G, target_node, n_layer + 2)\n",
    "        for t in target_layers:\n",
    "            # TODO some text may include '_' may change a method to get features in the future version\n",
    "            if len(t.split('_')) != 3: continue\n",
    "            word, layer, pos = t.split('_')\n",
    "            if int(pos) > ans_pos: \n",
    "                tmp_t = f'{word}_{layer}_{pos}_1'\n",
    "            else:tmp_t = f'{word}_{layer}_{pos}_0'\n",
    "            if tmp_t in repeat_check:continue\n",
    "            repeat_check.add(tmp_t)\n",
    "            if int(pos) > ans_pos and int(pos) not in marked_pos:\n",
    "                marked_pos.append(int(pos))\n",
    "    \n",
    "    del graph\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "    # print(marked_pos)\n",
    "    return marked_pos, repeat_check, result_graph_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274cba2",
   "metadata": {},
   "source": [
    "### Step3.4. collect the attribution graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17142a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "    current_idx:0\n",
      "interpret token idx:1, marked pos lenths:9\n",
      "interpret token idx:2, marked pos lenths:12\n",
      "interpret token idx:3, marked pos lenths:13\n",
      "interpret token idx:4, marked pos lenths:13\n",
      "interpret token idx:5, marked pos lenths:16\n",
      "interpret token idx:6, marked pos lenths:17\n",
      "interpret token idx:7, marked pos lenths:18\n",
      "interpret token idx:8, marked pos lenths:19\n",
      "interpret token idx:9, marked pos lenths:20\n",
      "interpret token idx:10, marked pos lenths:22\n",
      "interpret token idx:11, marked pos lenths:24\n",
      "interpret token idx:12, marked pos lenths:25\n",
      "interpret token idx:13, marked pos lenths:26\n",
      "interpret token idx:14, marked pos lenths:27\n",
      "interpret token idx:15, marked pos lenths:28\n",
      "interpret token idx:16, marked pos lenths:31\n",
      "interpret token idx:17, marked pos lenths:34\n",
      "interpret token idx:18, marked pos lenths:35\n",
      "interpret token idx:19, marked pos lenths:36\n",
      "interpret token idx:20, marked pos lenths:38\n",
      "interpret token idx:21, marked pos lenths:42\n",
      "interpret token idx:22, marked pos lenths:42\n",
      "interpret token idx:23, marked pos lenths:43\n",
      "interpret token idx:24, marked pos lenths:44\n",
      "interpret token idx:25, marked pos lenths:46\n",
      "interpret token idx:26, marked pos lenths:48\n",
      "interpret token idx:27, marked pos lenths:50\n",
      "interpret token idx:28, marked pos lenths:51\n",
      "interpret token idx:29, marked pos lenths:51\n",
      "interpret token idx:30, marked pos lenths:54\n",
      "interpret token idx:31, marked pos lenths:56\n",
      "interpret token idx:32, marked pos lenths:56\n",
      "interpret token idx:33, marked pos lenths:57\n",
      "interpret token idx:34, marked pos lenths:57\n",
      "interpret token idx:35, marked pos lenths:59\n",
      "interpret token idx:36, marked pos lenths:61\n",
      "interpret token idx:37, marked pos lenths:61\n",
      "interpret token idx:38, marked pos lenths:63\n",
      "interpret token idx:39, marked pos lenths:64\n",
      "interpret token idx:40, marked pos lenths:65\n",
      "interpret token idx:41, marked pos lenths:66\n",
      "interpret token idx:42, marked pos lenths:67\n",
      "interpret token idx:43, marked pos lenths:68\n",
      "interpret token idx:44, marked pos lenths:69\n",
      "interpret token idx:45, marked pos lenths:70\n",
      "interpret token idx:46, marked pos lenths:70\n",
      "interpret token idx:47, marked pos lenths:72\n",
      "interpret token idx:48, marked pos lenths:73\n",
      "interpret token idx:49, marked pos lenths:74\n",
      "interpret token idx:50, marked pos lenths:74\n",
      "interpret token idx:51, marked pos lenths:74\n",
      "interpret token idx:52, marked pos lenths:76\n",
      "interpret token idx:53, marked pos lenths:77\n",
      "interpret token idx:54, marked pos lenths:78\n",
      "interpret token idx:55, marked pos lenths:79\n",
      "interpret token idx:56, marked pos lenths:80\n",
      "interpret token idx:57, marked pos lenths:81\n",
      "interpret token idx:58, marked pos lenths:82\n",
      "interpret token idx:59, marked pos lenths:83\n",
      "interpret token idx:60, marked pos lenths:86\n",
      "interpret token idx:61, marked pos lenths:86\n",
      "interpret token idx:62, marked pos lenths:87\n",
      "interpret token idx:63, marked pos lenths:89\n",
      "interpret token idx:64, marked pos lenths:91\n",
      "interpret token idx:65, marked pos lenths:91\n",
      "interpret token idx:66, marked pos lenths:92\n",
      "interpret token idx:67, marked pos lenths:92\n",
      "interpret token idx:68, marked pos lenths:92\n",
      "interpret token idx:69, marked pos lenths:93\n",
      "interpret token idx:70, marked pos lenths:94\n",
      "interpret token idx:71, marked pos lenths:95\n",
      "interpret token idx:72, marked pos lenths:96\n",
      "interpret token idx:73, marked pos lenths:98\n",
      "interpret token idx:74, marked pos lenths:99\n",
      "interpret token idx:75, marked pos lenths:100\n",
      "interpret token idx:76, marked pos lenths:102\n",
      "interpret token idx:77, marked pos lenths:104\n",
      "interpret token idx:78, marked pos lenths:104\n",
      "interpret token idx:79, marked pos lenths:105\n",
      "interpret token idx:80, marked pos lenths:107\n",
      "interpret token idx:81, marked pos lenths:109\n",
      "interpret token idx:82, marked pos lenths:110\n",
      "interpret token idx:83, marked pos lenths:111\n",
      "interpret token idx:84, marked pos lenths:112\n",
      "interpret token idx:85, marked pos lenths:112\n",
      "interpret token idx:86, marked pos lenths:114\n",
      "interpret token idx:87, marked pos lenths:115\n",
      "interpret token idx:88, marked pos lenths:115\n",
      "interpret token idx:89, marked pos lenths:116\n",
      "interpret token idx:90, marked pos lenths:117\n",
      "interpret token idx:91, marked pos lenths:118\n",
      "interpret token idx:92, marked pos lenths:120\n",
      "interpret token idx:93, marked pos lenths:120\n",
      "interpret token idx:94, marked pos lenths:120\n",
      "interpret token idx:95, marked pos lenths:121\n",
      "interpret token idx:96, marked pos lenths:121\n",
      "interpret token idx:97, marked pos lenths:122\n",
      "interpret token idx:98, marked pos lenths:125\n",
      "interpret token idx:99, marked pos lenths:127\n",
      "interpret token idx:100, marked pos lenths:130\n",
      "interpret token idx:101, marked pos lenths:131\n",
      "interpret token idx:102, marked pos lenths:133\n",
      "interpret token idx:103, marked pos lenths:136\n",
      "interpret token idx:104, marked pos lenths:136\n",
      "interpret token idx:105, marked pos lenths:136\n",
      "interpret token idx:106, marked pos lenths:138\n",
      "interpret token idx:107, marked pos lenths:141\n",
      "interpret token idx:108, marked pos lenths:141\n",
      "interpret token idx:109, marked pos lenths:141\n",
      "interpret token idx:110, marked pos lenths:143\n",
      "interpret token idx:111, marked pos lenths:143\n",
      "interpret token idx:112, marked pos lenths:143\n",
      "interpret token idx:113, marked pos lenths:144\n",
      "interpret token idx:114, marked pos lenths:144\n",
      "interpret token idx:115, marked pos lenths:145\n",
      "interpret token idx:116, marked pos lenths:146\n",
      "interpret token idx:117, marked pos lenths:146\n",
      "interpret token idx:118, marked pos lenths:147\n",
      "interpret token idx:119, marked pos lenths:147\n",
      "interpret token idx:120, marked pos lenths:148\n",
      "interpret token idx:121, marked pos lenths:148\n",
      "interpret token idx:122, marked pos lenths:149\n",
      "interpret token idx:123, marked pos lenths:150\n",
      "interpret token idx:124, marked pos lenths:150\n",
      "interpret token idx:125, marked pos lenths:150\n",
      "interpret token idx:126, marked pos lenths:150\n",
      "interpret token idx:127, marked pos lenths:151\n",
      "interpret token idx:128, marked pos lenths:152\n",
      "interpret token idx:129, marked pos lenths:154\n",
      "interpret token idx:130, marked pos lenths:154\n",
      "interpret token idx:131, marked pos lenths:154\n",
      "interpret token idx:132, marked pos lenths:154\n",
      "interpret token idx:133, marked pos lenths:156\n",
      "interpret token idx:134, marked pos lenths:157\n",
      "interpret token idx:135, marked pos lenths:158\n",
      "interpret token idx:136, marked pos lenths:158\n",
      "interpret token idx:137, marked pos lenths:160\n",
      "interpret token idx:138, marked pos lenths:161\n",
      "interpret token idx:139, marked pos lenths:161\n",
      "interpret token idx:140, marked pos lenths:161\n",
      "interpret token idx:141, marked pos lenths:162\n",
      "interpret token idx:142, marked pos lenths:163\n",
      "interpret token idx:143, marked pos lenths:163\n",
      "interpret token idx:144, marked pos lenths:163\n",
      "interpret token idx:145, marked pos lenths:165\n",
      "interpret token idx:146, marked pos lenths:165\n",
      "interpret token idx:147, marked pos lenths:165\n",
      "interpret token idx:148, marked pos lenths:166\n",
      "interpret token idx:149, marked pos lenths:166\n",
      "interpret token idx:150, marked pos lenths:166\n",
      "interpret token idx:151, marked pos lenths:167\n",
      "interpret token idx:152, marked pos lenths:167\n",
      "interpret token idx:153, marked pos lenths:167\n",
      "interpret token idx:154, marked pos lenths:167\n",
      "interpret token idx:155, marked pos lenths:168\n",
      "interpret token idx:156, marked pos lenths:169\n",
      "interpret token idx:157, marked pos lenths:169\n",
      "interpret token idx:158, marked pos lenths:170\n",
      "interpret token idx:159, marked pos lenths:170\n",
      "interpret token idx:160, marked pos lenths:171\n",
      "interpret token idx:161, marked pos lenths:172\n",
      "interpret token idx:162, marked pos lenths:172\n",
      "interpret token idx:163, marked pos lenths:174\n",
      "interpret token idx:164, marked pos lenths:174\n",
      "interpret token idx:165, marked pos lenths:174\n",
      "interpret token idx:166, marked pos lenths:175\n",
      "interpret token idx:167, marked pos lenths:176\n",
      "interpret token idx:168, marked pos lenths:177\n",
      "interpret token idx:169, marked pos lenths:177\n",
      "interpret token idx:170, marked pos lenths:179\n",
      "interpret token idx:171, marked pos lenths:180\n",
      "interpret token idx:172, marked pos lenths:182\n",
      "interpret token idx:173, marked pos lenths:184\n",
      "interpret token idx:174, marked pos lenths:184\n",
      "interpret token idx:175, marked pos lenths:185\n",
      "interpret token idx:176, marked pos lenths:185\n",
      "interpret token idx:177, marked pos lenths:185\n",
      "interpret token idx:178, marked pos lenths:185\n",
      "interpret token idx:179, marked pos lenths:185\n",
      "interpret token idx:180, marked pos lenths:186\n",
      "interpret token idx:181, marked pos lenths:187\n",
      "interpret token idx:182, marked pos lenths:187\n",
      "interpret token idx:183, marked pos lenths:187\n",
      "interpret token idx:184, marked pos lenths:187\n",
      "interpret token idx:185, marked pos lenths:187\n",
      "interpret token idx:186, marked pos lenths:187\n",
      "interpret token idx:187, marked pos lenths:188\n",
      "interpret token idx:188, marked pos lenths:189\n",
      "interpret token idx:189, marked pos lenths:189\n",
      "idx:0, marked:189\n"
     ]
    }
   ],
   "source": [
    "max_n_logits = 10   \n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 2048  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=128\n",
    "offload = 'cpu'# 'disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True \n",
    "\n",
    "count = 0\n",
    "\n",
    "print(len(test_data))\n",
    "\n",
    "# ⚠️ Warning: change you own idx range in test set\n",
    "# Here we show generate attribution graph for onw sample.\n",
    "# Also remember to save the graphs as suggested in collecting_attribution_graph.py\n",
    "start = 0\n",
    "end = 1\n",
    "test_data = test_data[start:end]\n",
    "\n",
    "tokenizer_Answer = tokenizer('Answer')['input_ids'][0]\n",
    "\n",
    "for idx, data in enumerate(test_data):\n",
    "    # files = [int(f.split('.')[0]) for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f))]\n",
    "    real_id = start + idx\n",
    "\n",
    "    for node_thre in [node_ratio]:\n",
    "        overall_acc = []\n",
    "        layer_counts = {}\n",
    "        counts = 0\n",
    "        \n",
    "        prompt = \"Question: \" + data['question'] + \"\\nLet's think step by step\\nAnswer:\\n\"\n",
    "        ans = data['ans']\n",
    "        \n",
    "        try:\n",
    "            tokens = tokenizer(ans, return_offsets_mapping=True)\n",
    "            offsets = tokens['offset_mapping']\n",
    "            ids = tokens['input_ids']\n",
    "            ans_position = ids.index(tokenizer_Answer)\n",
    "            position_lookup = {}\n",
    "            if real_id not in match_collect: continue\n",
    "            last_pos_ans = ans.rfind(match_collect[real_id])\n",
    "            ans_token = tokenizer(ans[:last_pos_ans])['input_ids']\n",
    "            dicts = {}\n",
    "            print(f\"    current_idx:{idx}\")\n",
    "            marked_pos, repeat_check, graph = implicit_route(ans[:last_pos_ans], ans_position)\n",
    "            dicts['**ans**'] = match_collect[real_id]\n",
    "            dicts[ans[last_pos_ans:last_pos_ans+1]] = {}\n",
    "            dicts[ans[last_pos_ans:last_pos_ans+1]]['last'] = repeat_check\n",
    "            graph_list = [graph]\n",
    "\n",
    "            i = 0\n",
    "            while i < len(marked_pos):\n",
    "                m = marked_pos[i]\n",
    "                words = ans[offsets[m][0]:offsets[m][1]]\n",
    "                \n",
    "                if words not in dicts:\n",
    "                    dicts[words] = {}\n",
    "                new_marked_pos, new_repeat_check, new_graph = implicit_route(ans[:offsets[m][0]], ans_position)\n",
    "                dicts[words][m] = new_repeat_check\n",
    "                graph_list.append(new_graph)\n",
    "                added = 0\n",
    "                for new_m in new_marked_pos:\n",
    "                    if new_m not in marked_pos:\n",
    "                        marked_pos.append(new_m)\n",
    "                        added += 1  \n",
    "                \n",
    "                i += 1\n",
    "                marked_pos_lens = len(marked_pos)\n",
    "                print(f\"interpret token idx:{i}, marked pos lenths:{marked_pos_lens}\")\n",
    "                \n",
    "            print(f\"idx:{idx}, marked:{len(marked_pos)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"Error message:\", e)           \n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
